{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"Spark Intro\"\n",
    "author: \"Salvo Nicotra\"\n",
    "format: \n",
    "    revealjs:\n",
    "        width: 1280\n",
    "        heigth: 800\n",
    "incremental: true  \n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "theme: white\n",
    "chalkboard: true\n",
    "css: style.css\n",
    "smaller: true\n",
    "scrollable: true\n",
    "include-before: |\n",
    "    <img src=\"images/unict-logo.png\" class=\"custom-logo\" alt=\"Logo\">\n",
    "include-after: |\n",
    "      <div class=\"custom-footer\">\n",
    "        *** Technologies for advanced programming (TAP) - 2024/2025 ***\n",
    "      </div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Apache Spark{background-color=\"white\" background-image=\"https://spark.apache.org/images/spark-logo-trademark.png\" background-size=\"30%\" background-opacity=\"0.5\" background-position=\"top\"}\n",
    "\n",
    "Unified engine for large-scale data analytics\n",
    "\n",
    "Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## The Apache Spark project's History\n",
    "[A Gentle Introduction to Apache Spark on Databricks](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055043/484361/latest.html)\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "Spark was originally written by the founders of Databricks during their time at UC Berkeley. \n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://i.imgflip.com/8o8ag0.jpg)\n",
    "[Nicsmeme](https://imgflip.com/i/8o8ag0)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Spark Trends\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "The Spark project started in 2009, was open sourced in 2010, and in 2013 its code was donated to Apache, becoming Apache Spark. \n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](images/SparkTrends.png){.lightbox}\n",
    "\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Open Source\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "The employees of Databricks have written over 75% of the code in Apache Spark and have contributed more than 10 times more code than any other organization.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://www.databricks.com/en-website-assets/static/fc5d7c6f8bc8dab49fefccf3e36b9f7f/19830.png)\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Distributed system\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Apache Spark is a sophisticated distributed computation framework for executing code in parallel across many different machines. \n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](images/dash.png)\n",
    "<https://www.linkedin.com/pulse/exploring-world-distributed-computing-frameworks-empowering-nath/>\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## In the Cloud\n",
    "**everyone** sells Spark as a service in Cloud\n",
    "\n",
    "- https://cloud.google.com/solutions/spark\n",
    "- https://aws.amazon.com/it/emr/features/spark/\n",
    "- https://learn.microsoft.com/it-it/azure/hdinsight/spark/apache-spark-overview\n",
    "- https://www.oracle.com/it/big-data/data-flow/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Genesis of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hadoop (2004)\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Doug Cutting:\n",
    "\n",
    "> The name my kid gave a stuffed yellow elephant. Short, relatively easy to spell and pronounce, meaningless, and not used elsewhere: those are my naming criteria. Kids are good at generating such. Googol is a kid’s term” Hadoop is hardly the first unusual name to be attached to a tech company, of course. Google was born from a misspelling of \"googol\" (1 followed by 100 zeros), which itself was invented when a mathematician was playing with his nephew and together they came up with a name for really big numbers.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](images/doug.png)\n",
    "\n",
    "[Source](https://www.cnbc.com/id/100769719)\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Keep it simple\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://minimalistquotes.com/wp-content/uploads/2022/08/simple-things-should-be-simple-and-complex-things-.jpg)\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "The question then became:\n",
    "there a way to make Hadoop and MR simpler and faster?\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Development History\n",
    "- Spark’s Early Years at AMPLab (2009)\n",
    "- First Paper 10-20x faster then map reduce (2010)\n",
    "- Spark 1.0 Released (2014)\n",
    "- Spark 2.0: Unifying DataFrame and Dataset. Structured Streaming (2016)\n",
    "- Spark 3.0: Hadoop 3.0 support, Support for Pandas, SQL Engine Faster (2020)\n",
    "- Spark 3.4: [Introducing Spark Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html) (2023)\n",
    "- Spark 3.5: Spark Connect GA, distributed training with [DeepSpeed](https://www.deepspeed.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## At the end\n",
    "\n",
    "![](https://i.imgflip.com/7dcyqm.jpg)\n",
    "[NicsMeme](https://imgflip.com/i/7dcyqm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Design Philosophy\n",
    "<https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch01.html>\n",
    "\n",
    "Spark’s design philosophy centers around four key characteristics:\n",
    "\n",
    "- Speed\n",
    "- Ease of use\n",
    "- Modularity\n",
    "- Extensibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Speed\n",
    "![](https://cc-media-foxit.fichub.com/image/fox-it-mondofox/0177f439-3c0f-44ae-9803-c25f8bfac0dd/flash-vs-superman-game-2jpg-maxw-824.jpg)\n",
    "\n",
    "<https://www.reddit.com/r/DCcomics/comments/271ueb/the_definitive_answer_to_flash_vs_superman/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Run workloads 100x faster.\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![Logistic Regression](https://spark.apache.org/images/logistic-regression.png)\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Apache Spark achieves:\n",
    "\n",
    "- high performance for both batch and streaming data\n",
    "- using a state-of-the-art DAG scheduler\n",
    "- a query optimizer\n",
    "- a physical execution engine.\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Why Spark is faster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 1. Hardware improvements\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Today’s commodity servers come cheap, with hundreds of gigabytes of memory, multiple cores, and the underlying Unix-based operating system taking advantage of efficient multithreading and parallel processing.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://5.imimg.com/data5/SELLER/Default/2023/10/353578866/VS/YQ/LQ/200119173/top-quality-trimmed-gold-ram-finger-scrap-5-tons-500x500.jpg)\n",
    "[RAM SCRAP](https://m.indiamart.com/proddetail/top-quality-trimmed-gold-ram-finger-scrap-5-tons-2852693519773.html)\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 2. Direct Acyclic Graph (DAG) Scheduler and Query Optimizer\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Provides an efficient computational graph that can usually be decomposed into tasks that are executed in parallel across workers on the cluster.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://www.researchgate.net/publication/336769100/figure/fig2/AS:817393752371221@1571893265396/Spark-DAG-for-a-WordCount-application-with-two-stages-each-consisting-of-three-tasks.png)\n",
    "\n",
    "<https://www.researchgate.net/publication/336769100_Artificial_neural_networks_based_techniques_for_anomaly_detection_in_Apache_Spark>\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Ease of Use\n",
    "![](http://www.quickmeme.com/img/4d/4d4759d82ce65de86834ff151bc8b419f89f4e2f0d003f10a54b236785e3e6d2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Modularity\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- Write applications quickly in Java, Scala, Python, R, and SQL.\n",
    "- Spark offers over 80 high-level operators that make it easy to build parallel apps. \n",
    "- And you can use it **interactively** from the Scala, Python, R, and SQL shells.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Scala Example**\n",
    "\n",
    "```scala\n",
    "df = spark.read.json(\"logs.json\") \n",
    "df.where(\"age > 21\").select(\"name.first\").show()\n",
    "```\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4. Generality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Combine SQL, streaming, and complex analytics.\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Spark powers a stack of libraries including:\n",
    "- **Spark SQL** module for working with structured data\n",
    "- **Spark Streaming** build streaming applications and pipelines\n",
    "- **MLlib** scalable machine learning library\n",
    "- **GraphX** API for graphs and graph-parallel computation\n",
    "New:\n",
    "- **Pandas API**: Use pandas syntax on Spark\n",
    "- **Spark Connect**: Client application that communicate with remote Spark server\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/spark-stack.png)\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Runs everywhere\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://images2.corriereobjects.it/methode_image/socialshare/2014/10/07/f143a1aa-4e22-11e4-b38c-5070a4632162.jpg)\n",
    "\n",
    "<https://www.corriere.it/foto-gallery/esteri/14_ottobre_07/nuovo-attrezzo-fare-sport-ruota-criceti-misura-d-uomo-809cb22a-4e22-11e4-b38c-5070a4632162.shtml>\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "**Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud.**\n",
    "You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, on Mesos, or on Kubernetes\n",
    "![](https://spark.apache.org/images/spark-runs-everywhere.png)\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## It can access diverse external data sources\n",
    "\n",
    "::: {.fragment}\n",
    "Analyse\n",
    "Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "<https://spark.apache.org/docs/latest/rdd-programming-guide.html#external-datasets>\n",
    ":::\n",
    "\n",
    "::: {.fragment}\n",
    "#### Query\n",
    "Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data.\n",
    "\n",
    "<https://spark.apache.org/docs/latest/sql-data-sources.html>\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In short\n",
    "- Apache Spark is a fast and general-purpose cluster computing system.\n",
    "- It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\n",
    "- Supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark (2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch/streaming data\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Unify the processing of your data in batches and real-time streaming, using your preferred language: Python, SQL, Scala, Java or R.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/batch-sstreaming-data-icon.svg)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SQL analytics\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Execute fast, distributed ANSI SQL queries for dashboarding and ad-hoc reporting. Runs faster than most data warehouses.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/sql-analytics-icon.svg)\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data science at scale\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Perform Exploratory Data Analysis (EDA) on petabyte-scale data without having to resort to downsampling\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/data-science-scale-icon.svg)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine learning\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Train machine learning algorithms on a laptop and use the same code to scale to fault-tolerant clusters of thousands of machines.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/images/machine-learning-icon.svg)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running basic example of Spark in docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**nics vanilla**\n",
    "\n",
    "~~Download from https://spark.apache.org/downloads.html into spark/setup\n",
    "We are going to use Spark 3.4.0 Prebuilt for Hadoop 3.3 and later \n",
    "https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
    "~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**official image for spark**\n",
    "\n",
    "Since 27/06/23\n",
    "\n",
    "https://github.com/apache/spark-docker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SparkPI\n",
    "\n",
    "<https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py>\n",
    "\n",
    "Use Monte Carlo Method  <https://theabbie.github.io/blog/estimate-pi-using-random-numbers.html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Run sparkExamplePi.sh\n",
    "```bash\n",
    "docker run -it --rm apache/spark /opt/spark/bin/run-example SparkPi 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Spark Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Spark Shell](https://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell) provides a simple way to learn the API, as well as a powerful tool to analyze data interactively. \n",
    "\n",
    "It is available in either Scala (which runs on the Java VM and is thus a good way to use existing Java libraries) or Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Run a Scala Spark shell in docker\n",
    "```bash\n",
    "# change the path according to your setup, remember should be a fullpath\n",
    "docker run --hostname spark -p 4040 -it --rm -v /home/tap/tap-workspace/tap2024/spark/dataset:/tmp/dataset  apache/spark /opt/spark/bin/spark-shell\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark shell in action\n",
    "\n",
    "```\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "24/04/27 14:11:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Spark context Web UI available at http://spark:4040\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1714227106342).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 11.0.22)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Execute some commands\n",
    "```scala\n",
    "scala> val textFile=spark.read.textFile(\"file:///tmp/dataset/lotr_characters.csv\");\n",
    "scala> textFile\n",
    "scala> textFile.count();\n",
    "scala> textFile.first();\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Start Py Spark Docker\n",
    "```bash\n",
    "docker run --hostname spark -p 4040:4040 -it --rm -v /home/tap/tap-workspace/tap2024/spark/dataset:/tmp/dataset  apache/spark /opt/spark/bin/pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create a RDD from a python list\n",
    "```python\n",
    "# Create a list\n",
    "data = range(10000) \n",
    "# Create a RDD using parallelize. \n",
    "distData = sc.parallelize(data) \n",
    "# who is sc ?\n",
    "sc\n",
    "# and distData ?\n",
    "distData\n",
    "# Let's list\n",
    "distData.collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create a RDD from a text file\n",
    "```python\n",
    "# An RDD can be also created from external storage\n",
    "# textFile creates a RDD(String) (remember when we use spark.read.file)\n",
    "distFile = sc.textFile(\"/tmp/dataset/The Return Of The King_djvu.txt\") \n",
    "distFile\n",
    "\n",
    "# Take the first ones\n",
    "distFile.take(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A simple map-reduce function\n",
    "```python\n",
    "sizeOfBook=distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)\n",
    "sizeOfBook\n",
    "\n",
    "# Let's do in steps\n",
    "\n",
    "# First step for each line compute the lenght of the line \n",
    "mappa=distFile.map(lambda s: len(s))\n",
    "# Show some elements\n",
    "mappa.take(20)\n",
    "# Then sum all the elements of the RDD \n",
    "reduce=mappa.reduce(lambda a, b: a + b)\n",
    "reduce\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Key-Value Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Typically Spark operations work on RDDs containing any type of objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Few operations are only available on RDDs of key-value pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements by a key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In Python, these operations work on RDDs containing built-in Python [tuples](https://realpython.com/python-tuple/) such as (1, 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In pyspark create RDD of tuples and then call your desired operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A line count example\n",
    "```python\n",
    "# Create a RDD of pairs from file, \n",
    "pairs = distFile.map(lambda s: (s, 1))\n",
    "pairs\n",
    "# We have create a new RDD, let's see what it contains\n",
    "pairs.take(50)\n",
    "# Now we can use a reduce function, to count how may times the line appears in the document\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "counts.take(50)\n",
    "# Let's order by key\n",
    "ordered=counts.sortByKey()\n",
    "# and take ordered\n",
    "ordered.takeOrdered(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's do a better analysis\n",
    "Which is the most frequent word in the book ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Most frequent word in the book\n",
    "```python\n",
    "words=distFile.flatMap(lambda line:line.split(\" \"))\n",
    "words.take(100)\n",
    "# Great, let's assign a counter and then sum\n",
    "wordCounters=words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "wordCounters.take(10)\n",
    "# Ok I want to sort now\n",
    "wordsSorted=wordCounters.takeOrdered(200, key = lambda x: -x[1])\n",
    "wordsSorted\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2024 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
