{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Spark SQL, DataFrames and Datasets Guide\"\n",
    "author: \"Salvo Nicotra\"\n",
    "format: \n",
    "    revealjs:\n",
    "        width: 1280\n",
    "        heigth: 800\n",
    "incremental: true  \n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "theme: white\n",
    "chalkboard: true\n",
    "css: style.css\n",
    "smaller: true\n",
    "scrollable: true\n",
    "include-before: |\n",
    "    <img src=\"images/unict-logo.png\" class=\"custom-logo\" alt=\"Logo\">\n",
    "include-after: |\n",
    "      <div class=\"custom-footer\">\n",
    "        *** Technologies for advanced programming (TAP) - 2024/2025 ***\n",
    "      </div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Spark SQL, DataFrames and Datasets Guide\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Originally\n",
    "\n",
    "RDD was the primary user-facing API in Spark since its inception.\n",
    "\n",
    "At the core, an RDD is:\n",
    "\n",
    "- an immutable distributed collection of elements of your data,\n",
    "- partitioned across nodes in your cluster that can be operated in parallel\n",
    "- with a low-level API that offers transformations and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Is RDD enough ?\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://i.imgflip.com/566dsy.jpg)\n",
    "\n",
    "[NicsMeme](https://imgflip.com/i/566dsy)\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://i.imgflip.com/6eibrt.jpg)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark SQL\n",
    "\n",
    "- Spark SQL is a Spark module for **structured data processing**.\n",
    "- Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations.\n",
    "- There are several ways to interact with Spark SQL including **SQL** and the **Dataset API**.\n",
    "- When computing a result, the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structured data\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Tidy Dataset**\n",
    "\n",
    "> “Happy families are all alike; every unhappy family is unhappy in its own way.” –– Leo Tolstoy\n",
    "\n",
    "> “Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham\n",
    "\n",
    "<https://uta.pressbooks.pub/datanotebook/chapter/1-3-structured-data/>\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://uta.pressbooks.pub/app/uploads/sites/85/2021/05/Tidy-Data.png)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is structured data?\n",
    "<https://aws.amazon.com/what-is/structured-data/>\n",
    "\n",
    "- Structured data is data that has a standardized format for efficient access by software and humans alike.\n",
    "- It is typically tabular with rows and columns that clearly define data attributes.\n",
    "- Computers can effectively process structured data for insights due to its quantitative nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structured data in Data Science\n",
    "<https://multix.io/structured-data-module/docs/overview-structured-data.html>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- Goal 1: Connect steps in the structured data processing pipeline to a real-world case.\n",
    "- Goal 2: Preprocess structured data and prepare features/labels for modeling using pandas.\n",
    "- Goal 3: Understand how Principal Component Analysis can help explore data.\n",
    "- Goal 4: Understand how cross-validation works for time-series data.\n",
    "- Goal 5: Have a general understanding of Decision Tree and Random Forest.\n",
    "- Goal 6: Understand the concept of permutation feature importance.\n",
    "- Goal 7: Experiment with different feature sets and reflect on the choice of features.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://multix.io/structured-data-module/_images/smellpgh-pipeline.png)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SQL{background-color=\"white\" background-image=\"https://github.com/nicshub/sdsdbms/blob/main/images/sqladam.jpg?raw=true\" background-size=\"50%\" background-opacity=\"0.8\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Spark SQL\n",
    "- One use of Spark SQL is to execute SQL queries.\n",
    "- Spark SQL can also be used to read data from an existing Hive installation.\n",
    "- When running SQL from within another programming language the results will be returned as a Dataset/DataFrame. \n",
    "- You can also interact with the SQL interface using the command-line or over JDBC/ODBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "- A Dataset is a distributed collection of data. \n",
    "\n",
    "- Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. \n",
    "\n",
    "- A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). \n",
    "\n",
    "- The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Frame\n",
    "- A DataFrame is a Dataset organized into named columns. \n",
    "\n",
    "- It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. \n",
    "\n",
    "- DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. \n",
    "\n",
    "- The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Yet another definitions of DataFrames \n",
    "<https://www.databricks.com/glossary/what-are-dataframes>\n",
    "\n",
    "The concept of a DataFrame is common across many different languages and frameworks. DataFrames are the main data type used in pandas, the popular Python data analysis library, and DataFrames are also used in R, Scala, and other languages.\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- Every DataFrame contains a blueprint, known as a schema, that defines the name and data type of each column. \n",
    "- Spark DataFrames can contain universal data types like StringType and IntegerType, as well as data types that are specific to Spark, such as StructType. \n",
    "- Missing or incomplete values are stored as null values in the DataFrame.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2019/04/new-1-768x515.png)\n",
    "<https://www.edureka.co/blog/dataframes-in-spark/>\n",
    "\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Like a spreedsheet\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- A simple analogy is that a DataFrame is like a spreadsheet with named columns. \n",
    "- However, the difference between them is that while a spreadsheet sits on one computer in one specific location, a DataFrame can span thousands of computers. \n",
    "- In this way, DataFrames make it possible to do analytics on **big data**, using distributed computing clusters.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Visicalc.png/440px-Visicalc.png)\n",
    "\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### But distributed \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "The reason for putting the data on more than one computer should be intuitive: either the data is too large to fit on one machine or it would simply take too long to perform that computation on one machine.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://www.databricks.com/wp-content/uploads/2018/05/DataFrames.png)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## When to use RDDs?\n",
    " \n",
    "Consider these scenarios or common use cases for using RDDs when:\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Pro**\n",
    "\n",
    "- you want low-level transformation and actions and control on your dataset;\n",
    "- your data is unstructured, such as media streams or streams of text;\n",
    "- you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "\n",
    "::: \n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Contra**\n",
    "\n",
    "- you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column;\n",
    "- you can forgot some optimization and performance benefits available with DataFrames and Datasets for structured and semi-structured data.\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Dataframe efficency\n",
    "\n",
    "![](https://databricks.com/wp-content/uploads/2016/07/memory-usage-when-caching-datasets-vs-rdds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Evolution\n",
    "\n",
    "::: {.r-stack}\n",
    "![](images/human-evolution-monkey-modern-man-programmer-computer-user-isolated-white_33099-1593.jpg){.fragment}\n",
    "\n",
    "![](https://databricks.com/wp-content/uploads/2016/06/Unified-Apache-Spark-2.0-API-1.png){.fragment}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## When should I use DataFrames or RDD (2) ?\n",
    "\n",
    "- If you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame or Dataset.\n",
    "- If your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame or Dataset.\n",
    "- If you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "- If you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "- If you are a R user, use DataFrames.\n",
    "- If you are a Python user, use DataFrames and resort back to RDDs if you need more control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A nice comparison\n",
    "\n",
    "![](images/Apache-Spark-RDD-vs-DataFrame-vs-DataSet-1.jpg)\n",
    "\n",
    "<https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Get started with Spark SQL\n",
    "<https://spark.apache.org/docs/latest/sql-getting-started.html>\n",
    "\n",
    "Let's run and analyse two examples\n",
    "```bash \n",
    "\n",
    "docker run --hostname spark -p 4040:4040 -it --rm --workdir /opt/spark  apache/spark /opt/spark/bin/spark-submit examples/src/main/python/sql/basic.py\n",
    "\n",
    "docker run --hostname spark -p 4040:4040 -it --rm --workdir /opt/spark  apache/spark /opt/spark/bin/spark-submit --class org.apache.spark.examples.sql.SparkSQLExample --master local examples/jars/spark-examples.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos using spark shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a spark cluster\n",
    "- Submit a job\n",
    "- https://cloud.google.com/dataproc/docs/tutorials/gcs-connector-spark-tutorial?hl=it#python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Demos on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Data Frame Example\n",
    "<https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/3119543398385477/2956912205716139/latest.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Flights Example\n",
    "<https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1408031979081866/4241690966276695/2956912205716139/latest.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2024 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
