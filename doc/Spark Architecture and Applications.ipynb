{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Spark Architecture and Applications \"\n",
    "author: \"Salvo Nicotra\"\n",
    "format: \n",
    "    revealjs:\n",
    "        width: 1280\n",
    "        heigth: 800\n",
    "incremental: true  \n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "theme: white\n",
    "chalkboard: true\n",
    "css: style.css\n",
    "smaller: true\n",
    "scrollable: true\n",
    "include-before: |\n",
    "    <img src=\"images/unict-logo.png\" class=\"custom-logo\" alt=\"Logo\">\n",
    "include-after: |\n",
    "      <div class=\"custom-footer\">\n",
    "        *** Technologies for advanced programming (TAP) - 2024/2025 ***\n",
    "      </div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Architecture and Applications{background-color=\"black\" background-image=\"https://miro.medium.com/max/1400/1*arBqq7O7umskV4O7JjhdrA.jpeg\" background-size=\"75%\" background-opacity=\"0.5\" background-position=\"top\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Install pyspark locally\n",
    "\n",
    ":::{.fragment}\n",
    "Download Spark \n",
    "```bash\n",
    "wget https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
    "tar xzf spark-3.5.3-bin-hadoop3.tgz\n",
    "ln -s spark-3.5.3-bin-hadoop3 spark\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{.fragment}\n",
    "Install pyspark/findspark \n",
    "```bash\n",
    "pip install pyspark findspark\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{.fragment}\n",
    "Test on bash\n",
    "```bash\n",
    "export SPARK_HOME=/home/tap/spark\n",
    "$SPARK_HOME/bin/run-example SparkPi\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{.fragment}\n",
    "Test on jupyter\n",
    "```bash\n",
    "export SPARK_HOME=/home/tap/spark\n",
    "Run Jupyter\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/17 15:24:52 WARN Utils: Your hostname, pappanics resolves to a loopback address: 127.0.1.1; using 192.168.45.65 instead (on interface ens160)\n",
      "24/11/17 15:24:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/17 15:24:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.45.65:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Tap</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[8] appName=Tap>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAppName('Tap').setMaster('local[8]')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Spark Variables\n",
    "![](https://programmerhumor.io/wp-content/uploads/2023/02/programmerhumor-io-programming-memes-20822be2f46de63-608x776.png){.fragment .lightbox}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Shared Variables\n",
    "\n",
    "- Normally, when a function passed to a Spark operation (such as map or reduce) is executed on a remote cluster node, it works on separate copies of all the variables used in the function.\n",
    "- These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program.\n",
    "- Supporting general, read-write shared variables across tasks would be inefficient.\n",
    "- Spark does provide two limited types of shared variables for two common usage patterns: **broadcast variables** and **accumulators**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Broadcast\n",
    "- Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.\n",
    "- They can be used, for example, to give every node a copy of a large input dataset in an efficient manner.\n",
    "- Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\n",
    "- Spark actions are executed through a set of stages, separated by distributed “shuffle” operations.\n",
    "- Spark automatically broadcasts the common data needed by tasks within each stage.\n",
    "- The data broadcasted this way is cached in serialized form and deserialized before running each task.\n",
    "- This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.05"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euroRate = sc.broadcast(1.05)\n",
    "euroRate.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[92, 59, 68, 37, 76, 52, 21, 70, 67, 80]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "#Generate 5 random numbers between 1 and 100\n",
    "sales = random.sample(range(1, 100), 10)\n",
    "salesRDD=sc.parallelize(sales)\n",
    "salesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['96.6$',\n",
       " '61.95$',\n",
       " '71.4$',\n",
       " '38.85$',\n",
       " '79.8$',\n",
       " '54.6$',\n",
       " '22.05$',\n",
       " '73.5$',\n",
       " '70.35$',\n",
       " '84.0$']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use euroRate to get tle\n",
    "salesRDD.map(lambda x: str(round(x*euroRate.value,2))+\"$\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781.788424774219"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesRDD.reduce(lambda a,b:(a+b)*euroRate.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "##### Notes\n",
    "- After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).\n",
    "\n",
    "- To release the resources that the broadcast variable copied onto executors, call .unpersist(). If the broadcast is used again afterwards, it will be re-broadcast. To permanently release all resources used by the broadcast variable, call .destroy(). The broadcast variable can’t be used after that. Note that these methods do not block by default. To block until resources are freed, specify blocking=true when calling them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Accumulators\n",
    "- Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel.\n",
    "- They can be used to implement counters (as in MapReduce) or sums.\n",
    "- Spark natively supports accumulators of numeric types, and programmers can add support for new types.\n",
    "- For accumulator updates performed inside **actions** only, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value.\n",
    "- In **transformations**, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data=sc.parallelize([1, 2, 3, 4])\n",
    "data.foreach(lambda x: accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Accumulator and lazy evaluation**\n",
    "\n",
    "- Accumulators do not change the lazy evaluation model of Spark.\n",
    "- If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action.\n",
    "- Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like map(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=1, value=0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0) # Reset\n",
    "accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=1, value=0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def g(x):\n",
    "    accum.add(x)\n",
    "    return x\n",
    "data.map(g) \n",
    "# Here, accum is still 0 because no actions have caused the `map` to be computed.\n",
    "accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.map(g).foreach(lambda x: accum.add(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Which is the value of accum ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=1, value=14>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Spark Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "[Spark Documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "\n",
    "Spark application consists of a **driver** program that runs the user’s main function and executes various parallel operations on a cluster. \n",
    "\n",
    "Let's see the anatomy of a spark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![](https://miro.medium.com/max/700/1*B9lbB8uU7a_Xi0a1uDImRw.jpeg){.lightbox}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![](https://spark.apache.org/docs/latest/img/cluster-overview.png){.lightbox}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![[Anatomy](https://medium.com/@meenakshisundaramsekar/anatomy-of-a-spark-application-in-a-nutshell-2e542d5f334e)](images/anatomyofsparkapp.png){.lightbox}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Driver\n",
    "\n",
    "- The life of Spark programs starts and ends with the Spark Driver.\n",
    "- The Spark driver is the process which the clients used to submit the spark program.\n",
    "- The Driver is also responsible for application planning and execution of the spark program and returning the status/results to the client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "[Apache Spark Architeture](https://www.dezyre.com/article/apache-spark-architecture-explained-in-detail/338)\n",
    "![](images/sparkarchiteture.png)\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- It is the central point and the entry point of the Spark Shell (Scala, Python, and R).\n",
    "- The driver program runs the main() function of the application and is the place where the Spark Context is created.\n",
    "- Spark Driver contains various components responsible for the translation of spark user code into actual spark jobs executed on the cluster.\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## DAG Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### What is the DAG Scheduler (1)\n",
    "[SparkBasic](https://medium.com/@goyalsaurabh66/spark-basics-rdds-stages-tasks-and-dag-8da0f52f0454)\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling. \n",
    "\n",
    "- It transforms a logical execution plan (i.e. RDD lineage of dependencies built using RDD transformations) to a physical execution plan (using stages).\n",
    "\n",
    ":::{.fragment}\n",
    "```scala\n",
    "val input = sc.textFile(\"log.txt\")\n",
    "val splitedLines = input.map(line => line.split(\" \"))\n",
    ".map(words => (words(0), 1)).\n",
    "reduceByKey{(a,b) => a + b}\n",
    "```\n",
    ":::\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://miro.medium.com/max/700/1*1WfneX6c7Lc9fqAaR9MaGA.png)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### What is the DAG Scheduler (2)\n",
    "[SparkByExample](https://sparkbyexamples.com/spark/what-is-dag-in-spark/#:~:text=In%20Spark%2C%20the%20DAG%20Scheduler,across%20a%20cluster%20of%20machines.)\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"40%\"}\n",
    "In Spark, the DAG Scheduler is responsible for transforming a sequence of RDD transformations and actions into a directed acyclic graph (DAG) of stages and tasks, which can be executed in parallel across a cluster of machines. \n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"60%\"}\n",
    "1. **stages in Spark**: shuffle stages and non-shuffle stages. Shuffle stages involve the exchange of data between nodes, while non-shuffle stages do not.\n",
    "2. **Tasks**: A task represents a single unit of work that can be executed on a single partition of an RDD. Tasks are the smallest units of parallelism in Spark.\n",
    "3. **Dependencies**: The dependencies between RDDs determine the order in which tasks are executed.\n",
    "   - Narrow dependencies indicate that each partition of the parent RDD is used by at most one partition of the child RDD\n",
    "   - wide dependencies indicate that each partition of the parent RDD can be used by multiple partitions of the child RDD.\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## TaskScheduler\n",
    "<https://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/TaskScheduler.html>\n",
    "```javadoc\n",
    "public interface TaskScheduler\n",
    "```\n",
    "\n",
    "- Low-level task scheduler interface, currently implemented exclusively by TaskSchedulerImpl.\n",
    "- This interface allows plugging in different task schedulers.\n",
    "- Each TaskScheduler schedules tasks for a single SparkContext.\n",
    "- These schedulers get sets of tasks submitted to them from the DAGScheduler for each stage, and are responsible for sending the tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers.\n",
    "- They return events to the DAGScheduler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### what does it do ?\n",
    "[Source](https://mallikarjuna_g.gitbooks.io/spark/content/spark-taskscheduler.html)\n",
    "\n",
    "A TaskScheduler schedules tasks for a single Spark application according to scheduling mode.\n",
    "\n",
    "![](https://mallikarjuna_g.gitbooks.io/spark/content/images/sparkstandalone-sparkcontext-taskscheduler-schedulerbackend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## [SchedulerBackend](https://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulerBackend.html)\n",
    "\n",
    "::: {.r-stack}\n",
    "![](images/SparkSchedulerBackend1.png){.fragment .lightbox width=\"75%\"}\n",
    "\n",
    "![](images/SparkSchedulerBackend.png){.fragment .lightbox width=\"75%\"}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## BlockManager \n",
    "Spark storage system is managed by BlockManager that runs both in Driver and Executor instances.\n",
    "\n",
    "Is a key-value store of blocks of data (block storage) identified by a block ID.\n",
    "\n",
    "Among the types of data stored in blocks we can find:\n",
    "\n",
    "- RDD \n",
    "- shuffle: in this category we can distinguish shuffle data, shuffle index and temporary shuffle files (intermediate results)\n",
    "- broadcast - broadcasted data is organized in blocks too\n",
    "- task results\n",
    "- stream data\n",
    "- temp data (including swap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Repetita Iuvant\n",
    "<https://databricks.com/glossary/what-are-spark-applications>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"40%\"}\n",
    "![](https://databricks.com/wp-content/uploads/2018/05/Spark-Applications.png)\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"60%\"}\n",
    "The driver process:\n",
    "\n",
    "- runs your main() function\n",
    "- sits on a node in the cluster\n",
    "- is responsible for three things: \n",
    "   1. maintaining information about the Spark Application;\n",
    "   2. responding to a user’s program or input; \n",
    "   3. and analyzing, distributing, and scheduling work across the executors (defined momentarily).\n",
    "- The driver process is absolutely essential, it’s the heart of a Spark Application and maintains all relevant information during the lifetime of the application.\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Spark Context \n",
    "- The Spark context is application's Instance created by the Spark driver for each individual Spark programs when it is first submitted by the user.\n",
    "- Allows Spark Driver to access the cluster through a Cluster Resource Manager and it can be used to create RDDs, accumulators and broadcast variables on the cluster. Spark Context also keeps track of live executors by sending heartbeat messages regularly.\n",
    "- The Spark Context is created by the Spark Driver for each Spark application when it is first submitted by the user. It exists throughout the entire life of a spark application.\n",
    "- Usually referred to as variable name sc in programming.\n",
    "- The Spark Context terminates once the spark application completes. Only one Spark Context can be active per JVM. You must stop() the active Spark Context before creating a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Deploy, Package and Launch\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"33%\"}\n",
    "**Deploy**\n",
    "\n",
    "- The spark-submit script in Spark’s bin directory is used to launch applications on a cluster.\n",
    "- It can use all of Spark’s supported cluster managers through a uniform interface so you don’t have to configure your application specially for each one.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"33%\"}\n",
    "**Package**\n",
    "\n",
    "- Create an assembly jar (or “uber” jar) containing your code and its dependencies. Both sbt and Maven have assembly plugins (spark and Hadoop to be marked as provided dependencies)\n",
    "- Call the bin/spark-submit script as shown here while passing your jar.\n",
    "\n",
    ":::\n",
    "::: {.fragment .column width=\"33%\"}\n",
    "**Launch**\n",
    "\n",
    "- Once a user application is bundled, it can be launched using the bin/spark-submit script. \n",
    "\n",
    "- This script takes care of setting up the classpath with Spark and its dependencies, and can support different cluster managers and deploy modes that Spark supports:\n",
    "\n",
    "::: {.fragment}\n",
    "```bash\n",
    "./bin/spark-submit --class <mainclass>\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  <application-jar> [args]\n",
    "```\n",
    ":::\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example of app submit\n",
    "```bash\n",
    "# Run application locally on 8 cores\n",
    "# cd spark home\n",
    "./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[8] examples/jars/spark-examples_2.12-3.5.1.jar 1000\n",
    "\n",
    "```\n",
    "\n",
    "More in https://spark.apache.org/docs/latest/submitting-applications.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##  Spark Tap Docker\n",
    "- Code in tap/spark/code is copied into docker\n",
    "- Dataset is inside spark and linked into tap root (check previuos example), reference as spark/dataset...\n",
    "- [Hint] Test on machine and then test on Docker (including dependencies)\n",
    "- https://github.com/apache/spark/tree/master/examples/src/main/python is a good source\n",
    "\n",
    ":::{.fragment}\n",
    "```bash\n",
    "docker run --hostname spark -p 4040:4040 -it --rm \\\n",
    "-v /home/tap/tap-workspace/tap2024/spark/code/:/opt/tap/ \\\n",
    "-v /home/tap/tap-workspace/tap2024/spark/dataset:/tmp/dataset \\\n",
    "apache/spark /opt/spark/bin/spark-submit /opt/tap/simpleapp.py\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2024 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
