{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Apache Kafka - Streams\"\n",
    "author: \"Salvo Nicotra\"\n",
    "format: \n",
    "    revealjs:\n",
    "        width: 1280\n",
    "        heigth: 800\n",
    "incremental: true  \n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "theme: white\n",
    "chalkboard: true\n",
    "css: style.css\n",
    "smaller: true\n",
    "scrollable: true\n",
    "include-before: |\n",
    "    <img src=\"images/unict-logo.png\" class=\"custom-logo\" alt=\"Logo\">\n",
    "include-after: |\n",
    "      <div class=\"custom-footer\">\n",
    "        *** Technologies for advanced programming (TAP) - 2024/2025 ***\n",
    "      </div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Kafka Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## What is ?\n",
    "<https://kafka.apache.org/38/documentation/streams/>\n",
    "\n",
    "- Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters.\n",
    "- Kafka Streams combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka's server-side cluster technology to make these applications highly scalable, elastic, fault-tolerant, distributed, and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Core Concepts\n",
    "https://kafka.apache.org/35/documentation/streams/core-concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**It's a library**\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Designed as a **simple and lightweight client library**, which can be easily embedded in any Java application and integrated with any existing packaging, deployment and operational tools that users have for their streaming applications.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "i.e a dependency in a \n",
    "```xml\n",
    "<dependency>\n",
    "    <groupId>org.apache.kafka</groupId>\n",
    "    <artifactId>kafka-streams</artifactId>\n",
    "    <version>3.8.0</version>\n",
    "</dependency>\n",
    "```\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Standalone**\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Has **no external dependencies on systems other than Apache Kafka itself** as the internal messaging layer; notably, it uses Kafka's partitioning model to horizontally scale processing while maintaining strong ordering guarantees.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://cdn.confluent.io/wp-content/uploads/2016/08/consumer-group-2.png)\n",
    "<https://www.confluent.io/blog/elastic-scaling-in-kafka-streams/>\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**fault tolerant**\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- Supports **fault-tolerant local state**, which enables very fast and efficient stateful operations like windowed joins and aggregations.\n",
    "- State stores on single machines (RocksDB) combined with Kafka topics to back up and replicate the state, enabling quick recovery in case of instance failure. - -- This fault-tolerant design enables Kafka Streams applications to handle failures gracefully, ensuring uninterrupted processing and maintaining a consistent state across distributed systems.\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://kafka.apache.org/0102/images/streams-architecture-states.jpg)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Exactly Once Processing**\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Supports **exactly-once processing** semantics to guarantee that each record will be processed once and only once even when there is a failure on either Streams clients or Kafka brokers in the middle of processing.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "![](https://cdn.confluent.io/wp-content/uploads/kafka-topic.png)\n",
    "\n",
    "https://www.confluent.io/blog/enabling-exactly-once-kafka-streams/\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    ":::: {.columns}\n",
    "**One Recond at a time**\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Employs **one-record-at-a-time processing** to achieve millisecond processing latency, and supports **event-time based windowing operations** with out-of-order arrival of records.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://images.ctfassets.net/gt6dp23g0g38/y3dPJWV6inVi0KIWNk5Ic/5952ed5d7048e099a12ed57df173a39a/late-record-1.png)\n",
    "\n",
    "<https://developer.confluent.io/learn-kafka/kafka-streams/time-concepts/>\n",
    "\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Primitives**\n",
    "\n",
    "Offers necessary stream processing primitives, along with a **high-level Streams DSL** and a **low-level Processor API**.\n",
    "\n",
    "- High Level : <https://kafka.apache.org/documentation/streams/developer-guide/dsl-api.html>\n",
    "- Processor API: <https://kafka.apache.org/documentation/streams/developer-guide/processor-api.html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## The New York Times (2017)\n",
    "\n",
    "![](https://kafka.apache.org/images/powered-by/NYT.jpg){.lightbox}\n",
    "\n",
    "The New York Times uses Apache Kafka and the Kafka Streams to store and distribute, in real-time, published content to the various applications and systems that make it available to the readers.\n",
    "\n",
    "<https://open.nytimes.com/publishing-with-apache-kafka-at-the-new-york-times-7f0e3b7d2077>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Real Time Analytics\n",
    "\n",
    "![](https://dzone.com/storage/temp/12275703-kafka-use-case.png)\n",
    "\n",
    "Story: <https://dzone.com/articles/real-time-stream-processing-with-apache-kafka-part-1>\n",
    "\n",
    "Code: <https://github.com/hellosatish/microservice-patterns/tree/master/vehicle-tracker>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Wordcount Demo App\n",
    "``` bash\n",
    "# Build Kafka Stream (we will see the code later)\n",
    "cd kafka-stream\n",
    "./build.sh\n",
    "\n",
    "# Start Kafka Server\n",
    "docker run --rm -p 9092:9092 --network tap --name kafkaServer\\\n",
    " -e KAFKA_NODE_ID=1 \\\n",
    "  -e KAFKA_PROCESS_ROLES=broker,controller \\\n",
    "  -e KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \\\n",
    "  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafkaServer:9092 \\\n",
    "  -e KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER \\\n",
    "  -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \\\n",
    "  -e KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9093 \\\n",
    "  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\n",
    "  -e KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1 \\\n",
    "  -e KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1 \\\n",
    "  -e KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0 \\\n",
    "apache/kafka:3.8.0\n",
    "\n",
    "# Create the topic (required)\n",
    "docker exec -it --workdir /opt/kafka/bin/ kafkaServer ./kafka-topics.sh --create --bootstrap-server kafkaServer:9092  --topic streams-plaintext-input\n",
    "docker exec -it --workdir /opt/kafka/bin/ kafkaServer ./kafka-topics.sh --create --bootstrap-server kafkaServer:9092  --topic streams-wordcount-output\n",
    "\n",
    "# kafkaWordCountStream\n",
    "docker run -it --rm --network tap  tap:kafkastream java -cp /app/app.jar tap.WordCount\n",
    "\n",
    "# Start a producer \n",
    "docker exec --workdir /opt/kafka/bin/ -it kafkaServer ./kafka-console-producer.sh --topic streams-plaintext-input --bootstrap-server localhost:9092\n",
    "\n",
    "# In another tab open a consumer\n",
    "docker exec --workdir /opt/kafka/bin/ -it kafkaServer ./kafka-console-consumer.sh --topic streams-wordcount-output --from-beginning --bootstrap-server localhost:9092 --formatter kafka.tools.DefaultMessageFormatter --property print.key=true --property print.value=true --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer\n",
    "\n",
    "# Use \"All streams lead to Kafka\" followed by \"Hello kafka streams\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Behind the scenes\n",
    "\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://kafka.apache.org/34/images/streams-table-updates-01.png){.lightbox}\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://kafka.apache.org/34/images/streams-table-updates-02.png){.lightbox}\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Stream Processing Topology\n",
    "\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- A stream is the most important abstraction provided by Kafka Streams: it represents an unbounded, continuously updating data set. A stream is an ordered, replayable, and fault-tolerant sequence of immutable data records, where a data record is defined as a key-value pair.\n",
    "- A stream processing application is any program that makes use of the Kafka Streams library. It defines its computational logic through one or more processor topologies, where a processor topology is a graph of stream processors (nodes) that are connected by streams (edges).\n",
    "- A stream processor is a node in the processor topology; it represents a processing step to transform data in streams by receiving one input record at a time from its upstream processors in the topology, applying its operation to it, and may subsequently produce one or more output records to its downstream processors.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://kafka.apache.org/34/images/streams-architecture-topology.jpg)\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Duality of Streams and Tables\n",
    "\n",
    "Stream processing use cases in practice need both streams and also databases. \n",
    "\n",
    "An example use case that is very common in practice is an e-commerce application that enriches an incoming stream of customer transactions with the latest customer information from a database table.\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Stream as Table**: A stream can be considered a changelog of a table, where each data record in the stream captures a state change of the table. A stream is thus a table in disguise, and it can be easily turned into a \"real\" table by replaying the changelog from beginning to end to reconstruct the table. Similarly, in a more general analogy, aggregating data records in a stream - such as computing the total number of pageviews by user from a stream of pageview events - will return a table (here with the key and the value being the user and its corresponding pageview count, respectively).\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Table as Stream**: A table can be considered a snapshot, at a point in time, of the latest value for each key in a stream (a stream's data records are key-value pairs). A table is thus a stream in disguise, and it can be easily turned into a \"real\" stream by iterating over each key-value entry in the table.\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Stream and Tables: A Primer\n",
    "https://www.confluent.io/blog/kafka-streams-tables-part-1-event-streaming/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Event Records and Streams\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**An event records the fact that “something happened” in the world**\n",
    "\n",
    "- Event key: “Alice”\n",
    "- Event value: “Has arrived in Rome”\n",
    "- Event timestamp: “Dec. 3, 2019 at 9:06 a.m.”\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**An event stream records the history of what has happened in the world as a sequence of events**\n",
    "\n",
    "- This history is an ordered sequence or chain of events, so we know which event happened before another event to infer causality.\n",
    "\n",
    "- A stream thus represents both the past and the present: as we go from today to tomorrow—or from one millisecond to the next—new events are constantly being appended to the history.\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Example\n",
    "\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "_The sequence of moves in a chess match_\n",
    "\n",
    "White moved the e2 pawn to e4, then Black moved the e7 pawn to e5\n",
    "\n",
    "![](https://66.media.tumblr.com/tumblr_m8ok25dsch1r8gmlso1_500.gifv)\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "\n",
    "**A table represents the state of the world** at a particular point in time, typically “now.”\n",
    "\n",
    "![](https://cdn.confluent.io/wp-content/uploads/streams-vs-tables-1.png)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Stream vs Table\n",
    "\n",
    "| Stream | Table |\n",
    "| ------ | ----- |\n",
    "|A stream provides immutable data. It supports only inserting (appending) new events, whereas existing events cannot be changed. Streams are persistent, durable, and fault tolerant. Events in a stream can be keyed, and you can have many events for one key, like “all of Bob’s payments.” If you squint a bit, you could consider a stream to be like a table in a relational database (RDBMS) that has no unique key constraint and that is append only.| A table provides mutable data. New events—rows—can be inserted, and existing rows can be updated and deleted. Here, an event’s key aka row key identifies which row is being mutated. Like streams, tables are persistent, durable, and fault tolerant. Today, a table behaves much like an RDBMS materialized view because it is being changed automatically as soon as any of its input streams or tables change, rather than letting you directly run insert, update, or delete operations against it.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Repetita iuvant \n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "|                                           | Stream |  Table    |\n",
    "|-------------------------------------------|--------|-----------|\n",
    "| First event with key bob arrives          | Insert | Insert    |\n",
    "| Another event with key bob arrives        | Insert | Update    |\n",
    "| Event with key bob and value null arrives | Insert | Delete    |\n",
    "| Event with key null arrives               | Insert | _ignored_ |\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://cdn.confluent.io/wp-content/uploads/event-stream-1.gif)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Writing App\n",
    "https://docs.confluent.io/platform/current/streams/developer-guide/running-app.html\n",
    "\n",
    "```bash\n",
    "mvn archetype:generate \\\n",
    "    -DarchetypeGroupId=org.apache.kafka \\\n",
    "    -DarchetypeArtifactId=streams-quickstart-java \\\n",
    "    -DarchetypeVersion=3.8.0 \\\n",
    "    -DgroupId=streams.examples \\\n",
    "    -DartifactId=kafka-streams.examples \\\n",
    "    -Dversion=0.1 \\\n",
    "    -Dpackage=tap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2023 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
