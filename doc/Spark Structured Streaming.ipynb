{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Spark Structured Streams\"\n",
    "author: \"Salvo Nicotra\"\n",
    "format: \n",
    "    revealjs:\n",
    "        width: 1280\n",
    "        heigth: 800\n",
    "incremental: true  \n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "theme: white\n",
    "chalkboard: true\n",
    "css: style.css\n",
    "smaller: true\n",
    "scrollable: true\n",
    "include-before: |\n",
    "    <img src=\"images/unict-logo.png\" class=\"custom-logo\" alt=\"Logo\">\n",
    "include-after: |\n",
    "      <div class=\"custom-footer\">\n",
    "        *** Technologies for advanced programming (TAP) - 2024/2025 ***\n",
    "      </div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structured Streaming\n",
    "<https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html> \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Structured Streaming is \n",
    "\n",
    "- a scalable\n",
    "- fault-tolerant \n",
    "- **stream processing engine** \n",
    "- built on the **Spark SQL** engine. \n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://dbconvert.com/blog/content/images/size/w2000/2021/11/Data-stream-processing.png)\n",
    "\n",
    "<https://dbconvert.com/blog/data-stream-processing/>\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Same as batch\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- Streaming computation can be expressed in the the same way as a batch computation on static data. \n",
    "- The **Spark SQL engine** will run it **incrementally** and **continuously** and\n",
    "- update the final result as **streaming data** continues to arrive. \n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://images.ctfassets.net/8vofjvai1hpv/36gl5VFGguEw5PxZizOJVu/b50f5e031efda3635a0d1b912e106243/Database_Streaming.png)\n",
    "<https://www.confluent.io/learn/data-streaming/>\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### with same programming Languages\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Dataset/DataFrame API are avaiable in Scala, Java, Python or R, in addition of what we have seen already allow to manage:\n",
    "\n",
    "- streaming aggregations\n",
    "- event-time windows \n",
    "- stream-to-batch joins\n",
    "\n",
    "::: {.callout-tip .fragment}\n",
    "The computation is executed on the same optimized Spark SQL engine. \n",
    "This is an example of a callout with a title.\n",
    ":::\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DsRyFzVjioGKmpoRZW0VTg.png)\n",
    "\n",
    "<https://mohdizzy.medium.com/leverage-flink-windowing-to-process-streams-based-on-event-time-cdb87e9a1e21> \n",
    ":::\n",
    "::::\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### and some guarantees \n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Finally, the system ensures: \n",
    "\n",
    "- **end-to-end**\n",
    "- **exactly-once**\n",
    "- **fault-tolerance**\n",
    "- guarantees through **checkpointing** and **Write-Ahead Logs**\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://dimosr.github.io/assets/img/posts/exactly_once.jpg)\n",
    "\n",
    "<https://dimosr.github.io/the-tale-of-exactly-once-semantics/>\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In short\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Structured Streaming provides: \n",
    "\n",
    "- fast\n",
    "- scalable\n",
    "- fault-tolerant\n",
    "- end-to-end exactly-once \n",
    "\n",
    "stream processing without the user having to reason about streaming.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://i.imgflip.com/7kv31f.jpg)\n",
    "[NicsMeme](https://imgflip.com/i/7kv31f)\n",
    "\n",
    "A fake meme \n",
    "<https://screenrant.com/matrix-meme-what-told-you-not-in-movie/>\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### It's micro batch \n",
    "\n",
    ":::: {.columns}\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Queries are processed:\n",
    "\n",
    "- as small batch jobs, achieving 100 ms latency (and all gurantees)\n",
    "- with **Continuous Processing** (since Spark 2.3), lower to 1 ms (with at-least-once)\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://i.imgflip.com/9d1w9p.jpg)\n",
    "\n",
    "Inspired by [All that jazz](https://www.youtube.com/watch?v=xFK48dJXN_k)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Quick Example\n",
    "\n",
    "::: {.fragment}\n",
    "\n",
    "Let's run a Netcat server on port 9999\n",
    "\n",
    "```bash\n",
    "docker run -it --rm --network=tap --name tapnc  --hostname netcat  splooge/dnsutils nc -vl 9999\n",
    "```\n",
    "::: \n",
    "\n",
    "\n",
    "::: {.fragment}\n",
    "\n",
    "And a spark application \n",
    "\n",
    "```bash\n",
    "docker run --hostname spark --name sparkwc -p 4040:4040 --network tap -it --rm \\\n",
    "-v ./spark/code/:/opt/tap/ \\\n",
    "apache/spark \\\n",
    "/opt/spark/bin/spark-submit /opt/tap/structuredstreamingwc.py\n",
    "```\n",
    "\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Programming Model\n",
    "\n",
    "- **Key idea**: treat a live data stream as a table that is being continuously appended\n",
    "- to create a new stream processing model that is very similar to a batch processing model.\n",
    "- Streaming computation as standard batch-like query as on a static table\n",
    "- Spark runs it as an incremental query on the unbounded input table.\n",
    "\n",
    "::: {.fragment}\n",
    "```python\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "```\n",
    "::: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Input Table\n",
    "\n",
    "Data streams are \"inserted\" in the Input Table\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png){.fragment .r-scretch} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Query \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- Query the input will generate a \"Result table\".\n",
    "- Trigger interval \"updates\" the Result Table\n",
    "- Result rows can be exported to an external sink\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/docs/latest/img/structured-streaming-model.png) \n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Output\n",
    "\n",
    "Output is what will be sent to external storage\n",
    "\n",
    "- Complete Mode - The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle writing of the entire table.\n",
    "\n",
    "- Append Mode - Only the new rows appended in the Result Table since the last trigger will be written to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.\n",
    "\n",
    "- Update Mode - Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. If the query doesnâ€™t contain aggregations, it will be equivalent to Append mode.\n",
    "\n",
    "::: {.callout-note .fragment}\n",
    "Each mode can be applicable on specific types of queries\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Quick Example Model\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Notes \n",
    "\n",
    "- Structured Streaming does not materialize the entire table. It reads the latest available data from the streaming data source, processes it incrementally to update the result, and then discards the source data. \n",
    "\n",
    "- It only keeps around the minimal intermediate state data as required to update the result (e.g. intermediate counts in the earlier example).\n",
    "\n",
    "- This model is significantly different from many other stream processing engines. Many streaming systems require the user to maintain running aggregations themselves, thus having to reason about fault-tolerance, and data consistency (at-least-once, or at-most-once, or exactly-once). \n",
    "\n",
    "- In this model, Spark is responsible for updating the Result Table when there is new data, thus relieving the users from reasoning about it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## API\n",
    "\n",
    "A spark streaming Dataframe can be created in the Spark session using readStream\n",
    "\n",
    "```python\n",
    "spark = SparkSession...\n",
    "\n",
    "# Create DataFrame representing the stream of input lines \n",
    "lines = spark.readStream(INPUT)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Input Sources\n",
    "\n",
    "- **File** : Reads file (CSV, JSON, ORC, Parquet) in a directory (ordered by modification time) as stream of data\n",
    "- **Kafka**: Reads data from Kafka. [See Kafka Integration Guide](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n",
    "- **Socket**: (for testing purposes) reads UTF8 from a socket\n",
    "- **Rate**: (for testing purposes) generates data at the specified number of rows per seconds\n",
    "- **Rate Micro-Batch**: (for testing purposes) Generates data at the specified number of rows per micro-batch, each output row contains a timestamp and value. Unlike rate data source, this data source provides a consistent set of input rows per micro-batch regardless of query execution (configuration of trigger, query being lagging, etc.), say, batch 0 will produce 0-999 and batch 1 will produce 1000-1999, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Example: Streaming from file\n",
    "\n",
    "```bash\n",
    "docker run --hostname spark --name sparkwcr -p 4040:4040 --network tap -it --rm \\\n",
    "-v ./spark/code/:/opt/tap/ apache/spark \\\n",
    "/opt/spark/bin/spark-submit /opt/tap/structuredstreamingreadfile.py\n",
    "```\n",
    "\n",
    "Inside Docker\n",
    "```bash\n",
    "docker exec -it sparkwcr /bin/bash\n",
    "\n",
    "cd /opt/spark/work-dir\n",
    "echo \"nics,75\" > salvo.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Operations\n",
    "\n",
    "Streaming supports most of the operations on DataFrames/Datasets \n",
    "\n",
    "-  untyped, SQL-like operations (e.g. select, where, groupBy)\n",
    "-  typed RDD-like operations (e.g. map, filter, flatMap)\n",
    "\n",
    "[SQL programming guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Basic Operations - Selection, Projection, Aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Filters\n",
    "This example uses Rate Source as source, an UDF function to filter only prime numbers, prints in standard output \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "```python\n",
    "# Create DataFrame reading from Rate Source\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 100000) \\\n",
    "    .load()\n",
    "\n",
    "primes = df.filter(isprime('value'))\n",
    "\n",
    "# Start running the query \n",
    "# Prints the running counts to the console\n",
    "query = primes \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "```\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "```bash\n",
    "docker run --hostname spark --name spark \\ \n",
    "-p 4040:4040 --network tap -it --rm \\\n",
    "-v ./spark/code/:/opt/tap/ apache/spark \\\n",
    "/opt/spark/bin/spark-submit \\\n",
    "/opt/tap/structuredstreaming-prime-filters.py\n",
    "```\n",
    ":::\n",
    ":::: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Add new column\n",
    "Extends the previous example adding a new column with the last digit of the filtered signal\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "```python\n",
    "# Create DataFrame reading from Rate Source\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 100) \\\n",
    "    .load()\n",
    "\n",
    "#  Let's take the \"primes\" signals \n",
    "primes = df.filter(isprime('value'))\n",
    "\n",
    "# Adding a new column with tha last digit \n",
    "primes = primes.withColumn(\"last\",substring(\"value\",-1,1))\n",
    "\n",
    "# Start running the query \n",
    "# prints the running counts to the console\n",
    "query = primes \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "```\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "```bash\n",
    "docker run --hostname spark --name spark \\ \n",
    "-p 4040:4040 --network tap -it --rm \\\n",
    "-v ./spark/code/:/opt/tap/ apache/spark \\\n",
    "/opt/spark/bin/spark-submit \\\n",
    "/opt/tap/structuredstreaming-prime-newcol.py\n",
    "```\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### GroupBy \n",
    "\n",
    "Extends the example grouping by last digit and counting\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "```python\n",
    "# Create DataFrame reading from Rate Source\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 100) \\\n",
    "    .load()\n",
    "\n",
    "#  Let's take the \"primes\" signals \n",
    "primes = df.filter(isprime('value'))\n",
    "\n",
    "# Adding a new column with tha last digit \n",
    "primes = primes.withColumn(\"last\",substring(\"value\",-1,1))\n",
    "\n",
    "# Group By last \n",
    "primes = primes.groupBy(\"last\").count()\n",
    "\n",
    "# Start running the query \n",
    "# prints the running counts to the console\n",
    "query = primes \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "```\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "```bash\n",
    "docker run --hostname spark --name spark \\ \n",
    "-p 4040:4040 --network tap -it --rm \\\n",
    "-v ./spark/code/:/opt/tap/ apache/spark \\\n",
    "/opt/spark/bin/spark-submit \\\n",
    "/opt/tap/structuredstreaming-prime-groupBy.py\n",
    "```\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### SQL \n",
    "\n",
    "Add another query to count the number of incoming signals \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "```python\n",
    "# Create DataFrame reading from Rate Source\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 100) \\\n",
    "    .load()\n",
    "\n",
    "df.createOrReplaceTempView(\"signals\")\n",
    "signalsQuery=spark.sql(\"select count(*) from signals\")  \n",
    "# returns another streaming DF\n",
    "\n",
    "query2 = signalsQuery \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "```\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "```bash\n",
    "docker run --hostname spark --name spark \\ \n",
    "-p 4040:4040 --network tap -it --rm \\\n",
    "-v ./spark/code/:/opt/tap/ apache/spark \\\n",
    "/opt/spark/bin/spark-submit \\\n",
    "structuredstreaming-prime-groupByAndSQL.py\n",
    "```\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Window Operations on Event Time\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Aggregations over a sliding event-time window are very similar to grouped aggregations:\n",
    "\n",
    "- grouped aggregation: aggregate values (e.g. counts) are maintained for each unique value in the user-specified grouping column.\n",
    "- window-based aggregations: aggregate values are maintained for each window the event-time of a row falls into. \n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://spark.apache.org/docs/latest/img/structured-streaming-window.png){.lightbox}\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Window Example\n",
    "\n",
    "Start Spark Streaming \n",
    "\n",
    "```bash\n",
    "docker run --hostname spark --name spark -p 4040:4040 --network tap -it --rm \\\n",
    "-v ./spark/code/:/opt/tap/ apache/spark \\\n",
    "/opt/spark/bin/spark-submit \\\n",
    "/opt/tap/structured_network_wordcount_windowed.py localhost 9999 10 5\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Join Operations\n",
    "\n",
    "Structured Streaming supports incremental join of a streaming Dataset/DataFrame with:\n",
    "\n",
    "- static Dataset/DataFrame\n",
    "- streaming Dataset/DataFrame. \n",
    "\n",
    "::: {.callout-note}\n",
    "In all the supported join types, the result of the join with a streaming Dataset/DataFrame will be the exactly the same as if it was with a static Dataset/DataFrame containing the same data in the stream.\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stream-static Joins\n",
    "\n",
    "Since Spark 2.0, Structured Streaming has supported joins (inner join and some type of outer joins) between a streaming and a static DataFrame/Dataset.\n",
    "\n",
    "Example: Join a rate flow with a dataframe of numbers\n",
    "\n",
    "```bash\n",
    "docker run --hostname spark --name spark -p 4040:4040 --network tap -it --rm \\\n",
    "-v ./spark/code/:/opt/tap/ \\\n",
    "-v ./spark/dataset:/tmp/dataset \\\n",
    "apache/spark \\\n",
    "/opt/spark/bin/spark-submit /opt/tap/structuredstreaming-prime-join-static.py\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Stream-stream Joins\n",
    "\n",
    "Since  Spark 2.3. Structured Streaming has supported  some stream-stream joins. \n",
    "\n",
    "::: {.callout-warning}\n",
    "The challenge of generating join results between two data streams is that, at any point of time, the view of the dataset is incomplete for both sides of the join making it much harder to find matches between inputs. \n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Inner Joins with optional Watermarking\n",
    "\n",
    "- Inner joins on any kind of columns along with any kind of join conditions are supported.\n",
    "- To avoid unbounded state, since any new input can match with old input, additional join conditions need to be added\n",
    "\n",
    "::: {.fragment}\n",
    "**watermark**: Define watermark delays on both inputs such that the engine knows how delayed the input can be.\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment}\n",
    "**constraints**: Define a constraint on event-time across the two inputs such that the engine can figure out when old rows of one input is not going to be required for matches with the other input. \n",
    "\n",
    "This constraint can be defined in one of the two ways: \n",
    "\n",
    "1. Time range join conditions (```JOIN ON leftTime BETWEEN rightTime AND rightTime + INTERVAL 1 HOUR```)\n",
    "2. Join on event-time windows (```JOIN ON leftTimeWindow = rightTimeWindow```)\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "##### Example of Stream-Stream join\n",
    "\n",
    "Example of a ad stream using watermarking\n",
    "\n",
    "```bash\n",
    "docker run --hostname spark --name spark -p 4040:4040 --network tap -it --rm \\\n",
    "-v ./spark/code/:/opt/tap/ \\\n",
    "apache/spark \\\n",
    "/opt/spark/bin/spark-submit \\\n",
    "/opt/tap/structuredstreaming-join-streams-ad.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Starting Streaming Queries\n",
    "\n",
    "To start the streaming computation, a new DataStreamWriter ([Python docs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html#pyspark.sql.streaming.DataStreamWriter)) , returned through Dataset.writeStream(), needs to be created\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- Details of the output sink: Data format, location, etc.\n",
    "- Output mode: Specify what gets written to the output sink.\n",
    "- Query name: Optionally, specify a unique name of the query for identification.\n",
    "- Trigger interval: Optionally, specify the trigger interval. \n",
    "- Checkpoint location: For some output sinks where the end-to-end fault-tolerance can be guaranteed, specify the location where the system will write all the checkpoint information. This should be a directory in an HDFS-compatible fault-tolerant file system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Output Modes\n",
    "\n",
    "There are a few types of output modes.\n",
    "\n",
    "- Append mode (default) - This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant sink). For example, queries with only select, where, map, flatMap, filter, join, etc. will support Append mode.\n",
    "\n",
    "- Complete mode - The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries.\n",
    "\n",
    "- Update mode - (Available since Spark 2.1.1) Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. More information to be added in future releases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Output Sinks\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**File sink** - Stores the output to a directory.\n",
    "```scala\n",
    "writeStream // can be \"orc\", \"json\", \"csv\", etc.\n",
    ".format(\"parquet\")        \n",
    ".option(\"path\", \"path/to/destination/dir\")\n",
    ".start()\n",
    "```\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Kafka sink** - Stores the output to one or more topics in Kafka.\n",
    "```scala\n",
    "writeStream\n",
    ".format(\"kafka\")\n",
    ".option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    ".option(\"topic\", \"updates\")\n",
    ".start()\n",
    "```\n",
    ":::\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Foreach sink** - Runs arbitrary computation on the records in the output. See later in the section for more details.\n",
    "```scala\n",
    "writeStream\n",
    ".foreach(...)\n",
    ".start()\n",
    "```\n",
    ":::\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Console sink** (for debugging) - Prints the output to the console/stdout every time there is a trigger.\n",
    "```scala\n",
    "writeStream\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    "```\n",
    ":::\n",
    "::::\n",
    "\n",
    "::: {.callout-note}\n",
    "call start() to actually start the execution of the query, returns a StreamingQuery object which is a handle to the continuously running execution and usable to manage the query\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### For each/for each batch\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- ```foreachBatch(...)``` defines a function that is executed on the output data of every micro-batch of a streaming query. \n",
    "- function takes two parameters: a DataFrame or Dataset that has the output data of a micro-batch and the unique ID of the micro-batch.\n",
    "\n",
    "::: {.fragment}\n",
    "```python\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    # Transform and write batchDF\n",
    "    pass\n",
    "streamingDF.writeStream.foreachBatch(foreach_batch_function)\n",
    ".start()\n",
    "```\n",
    ":::\n",
    "\n",
    "::: {.callout-note .fragment}\n",
    "If foreachBatch is not an option (for example, corresponding batch data writer does not exist, or continuous processing mode), then you can express your custom writer logic using foreach. \n",
    "\n",
    ":::\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "1. **function**  simple way to express  processing logic, don't allow to deduplicate generated data when failures cause reprocessing of some input data.\n",
    "   \n",
    "::: {.fragment}\n",
    "```python\n",
    "def process_row(row):\n",
    "    # Write row to storage\n",
    "    pass\n",
    "query = streamingDF.writeStream.foreach(process_row).start()\n",
    "```\n",
    ":::\n",
    "2. **object** can define the methods open, process and close\n",
    "\n",
    "::: {.fragment}\n",
    "```python\n",
    "class ForeachWriter:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        # Open connection. This method is optional in Python.\n",
    "        pass\n",
    "\n",
    "    def process(self, row):\n",
    "        # Write row to connection. This method is NOT optional in Python.\n",
    "        pass\n",
    "\n",
    "    def close(self, error):\n",
    "        # Close the connection. This method in optional in Python.\n",
    "        pass\n",
    "      \n",
    "query = streamingDF.writeStream.foreach(ForeachWriter()).start()\n",
    "```\n",
    ":::\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Streaming Table APIs\n",
    "Since Spark 3.1, you can also use DataStreamReader.table() to read tables as streaming DataFrames and use DataStreamWriter.toTable() to write streaming DataFrames as tables:\n",
    "\n",
    "```python\n",
    "spark = ...  # spark session\n",
    "\n",
    "# Create a streaming DataFrame\n",
    "df = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .load()\n",
    "\n",
    "# Write the streaming DataFrame to a table\n",
    "df.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .toTable(\"myTable\")\n",
    "\n",
    "# Check the table result\n",
    "spark.read.table(\"myTable\").show()\n",
    "\n",
    "# Transform the source dataset and write to a new table\n",
    "spark.readStream \\\n",
    "    .table(\"myTable\") \\\n",
    "    .select(\"value\") \\\n",
    "    .writeStream \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .toTable(\"newTable\")\n",
    "\n",
    "# Check the new table result\n",
    "spark.read.table(\"newTable\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Not covered\n",
    "\n",
    "- triggers\n",
    "- managing streaming queries\n",
    "- Monitoring Streaming Queries\n",
    "- Reporting Metrics programmatically using Asynchronous APIs\n",
    "- Recovering from Failures with Checkpointing\n",
    "- Recovery Semantics after Changes in a Streaming Query\n",
    "- Continuous Processing\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2024 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
