{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Apache Kafka - Connect\"\n",
    "author: \"Salvo Nicotra\"\n",
    "format: \n",
    "    revealjs:\n",
    "        width: 1280\n",
    "        heigth: 800\n",
    "incremental: true  \n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "theme: white\n",
    "chalkboard: true\n",
    "css: style.css\n",
    "smaller: true\n",
    "scrollable: true\n",
    "include-before: |\n",
    "    <img src=\"images/unict-logo.png\" class=\"custom-logo\" alt=\"Logo\">\n",
    "include-after: |\n",
    "      <div class=\"custom-footer\">\n",
    "        *** Technologies for advanced programming (TAP) - 2024/2025 ***\n",
    "      </div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Kafka Connect{background-color=\"white\" background-image=\"https://images.ctfassets.net/gt6dp23g0g38/5vGOBwLiNaRedNyB0yaiIu/529a29a059d8971541309f7f57502dd2/ingest-data-upstream-systems.jpg\" background-size=\"80%\" background-opacity=\"0.5\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Overview\n",
    "<https://kafka.apache.org/documentation.html#connect_overview>\n",
    "\n",
    "- Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other systems.\n",
    "- Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency.\n",
    "- An export job can deliver data from Kafka topics into secondary storage and query systems or into batch systems for offline analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**A common framework for Kafka connectors** \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Kafka Connect standardizes integration of other data systems with Kafka, simplifying connector development, deployment, and management\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://images.ctfassets.net/8vofjvai1hpv/4io4iF1i7C6vaHt3w0EIal/b608a11ae2613cd91a226680c6796322/blog_IntroducingConfluentHub.png)\n",
    "<https://www.confluent.io/hub/>\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Distributed and standalone modes** \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Scale up to a large, centrally managed service supporting an entire organization or scale down to development, testing, and small production deployments\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://cdn.confluent.io/wp-content/uploads/kafka-connect-2.png)\n",
    "<https://www.confluent.io/blog/create-dynamic-kafka-connect-source-connectors/>\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**REST interface**\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Submit and manage connectors to your Kafka Connect cluster via an easy to use REST API\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://img.youtube.com/vi/4xWPDXhBi3g/maxresdefault.jpg)\n",
    "<https://developer.confluent.io/learn-kafka/kafka-connect/rest-api/>\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Automatic offset management** \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "With just a little information from connectors, Kafka Connect can manage the offset commit process automatically so connector developers do not need to worry about this error prone part of connector development\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "Examples File Source Connector:\n",
    "\n",
    "The File Source Connector uses offsets to keep track of the last byte position read in the file. As it reads new data, it periodically commits the latest byte position as the offset. If the connector restarts, it resumes reading from the last committed byte position. This prevents re-reading the entire file from the beginning, which could lead to duplicate data in Kafka.\n",
    "\n",
    "[See Source Code]( https://github.com/a0x8o/kafka/blob/master/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceTask.java)\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Distributed and scalable by default** \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Kafka Connect builds on the existing group management protocol. More workers can be added to scale up a Kafka Connect cluster.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "![](https://www.oreilly.com/api/v2/epubs/9781787122765/files/assets/842ab4a5-f79c-43d4-bd61-96b2eb53676a.png)\n",
    "<https://www.oreilly.com/library/view/modern-big-data/9781787122765/>\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Streaming/batch integration** \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Leveraging Kafka's existing capabilities, Kafka Connect is an ideal solution for bridging streaming and batch data systems\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://i.imgflip.com/7icsk4.jpg)\n",
    "[NicsMeme](https://imgflip.com/i/7icsk4)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Connect Standalone Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "## Start a kafka connect worker \n",
    "\n",
    "A worker can be run using the following command\n",
    "```bash\n",
    "> bin/connect-standalone.sh config/connect-standalone.properties [connector1.properties connector2.properties ...]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## File to File\n",
    "\n",
    "Goal: Create a kafka connect process that reads from a file and writes to another file.\n",
    "\n",
    "::: {.fragment}\n",
    "**worker properties**\n",
    "\n",
    "```properties\n",
    "bootstrap.servers=kafkaServer:9092\n",
    "key.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "key.converter.schemas.enable=true\n",
    "value.converter.schemas.enable=true\n",
    "offset.storage.file.filename=/tmp/connect.offsets\n",
    "offset.flush.interval.ms=10000\n",
    "plugin.path=/opt/kafka/libs/connect-file-3.8.0.jar\n",
    "```\n",
    "::: \n",
    "\n",
    "::: {.fragment}\n",
    "**source properties**\n",
    "\n",
    "```properties\n",
    "name=local-file-source\n",
    "connector.class=FileStreamSource\n",
    "tasks.max=1\n",
    "file=test.txt\n",
    "topic=connect-test\n",
    "```\n",
    "::: \n",
    "\n",
    "::: {.fragment}\n",
    "**sink properties**\n",
    "\n",
    "```properties\n",
    "name=local-file-sink\n",
    "connector.class=FileStreamSink\n",
    "tasks.max=1\n",
    "file=test.sink.txt\n",
    "topics=connect-test\n",
    "```\n",
    "::: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "```bash\n",
    "# GO in repo/kafka-connect dir\n",
    "cd kafka-connect\n",
    "\n",
    "# Start Kafka Server\n",
    "docker run --rm -p 9092:9092 --network tap --name kafkaServer -v $(pwd):/connect apache/kafka:3.8.0\n",
    "\n",
    "# Create the topic (optional)\n",
    "docker exec -it --workdir /opt/kafka/bin/ kafkaServer ./kafka-topics.sh --alter --bootstrap-server kafkaServer:9092  --topic connect-test --partitions 1\n",
    "\n",
    "# Start connect \n",
    "docker exec -it --workdir /opt/kafka/bin/ kafkaServer ./connect-standalone.sh /connect/connect-standalone.properties /connect/connect-file-source.properties /connect/connect-file-sink.properties\n",
    "\n",
    "# In another tab write to the source file\n",
    "docker exec -it kafkaServer sh -c \"echo hello > /tmp/my-test.txt\"\n",
    "\n",
    "# In another tab open a consumer\n",
    "docker exec --workdir /opt/kafka/bin/ -it kafkaServer ./kafka-console-consumer.sh --topic connect-test --from-beginning --bootstrap-server localhost:9092\n",
    "\n",
    "# In another tab run a tail on the destination  \n",
    "docker exec -it kafkaServer sh -c \"tail -f /tmp/test.sink.txt\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## API Rest\n",
    "https://kafka.apache.org/documentation/#connect_rest\n",
    "\n",
    "```bash\n",
    "# Open a new tab \n",
    "docker exec -it kafkaServer wget -qO /tmp/api  http://localhost:8083/connectors\n",
    "docker exec -it kafkaServer cat /tmp/api  \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2023 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
