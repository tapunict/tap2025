{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Apache Kafka\"\n",
    "author: \"Salvo Nicotra\"\n",
    "format: \n",
    "    revealjs:\n",
    "        width: 1280\n",
    "        heigth: 800\n",
    "incremental: true  \n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "theme: white\n",
    "chalkboard: true\n",
    "css: style.css\n",
    "smaller: true\n",
    "scrollable: true\n",
    "include-before: |\n",
    "    <img src=\"images/unict-logo.png\" class=\"custom-logo\" alt=\"Logo\">\n",
    "include-after: |\n",
    "      <div class=\"custom-footer\">\n",
    "        *** Technologies for advanced programming (TAP) - 2024/2025 ***\n",
    "      </div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kafka{background-color=\"black\" background-image=\"images/kafka-real.jpg\" background-size=\"100%\" background-opacity=\"0.5\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Origin of the name\n",
    "\n",
    "<https://www.quora.com/What-is-the-relation-between-Kafka-the-writer-and-Apache-Kafka-the-distributed-messaging-system>\n",
    "\n",
    "\n",
    "![](images/kafkaname.png){.fragment}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Kafka\n",
    "\n",
    "<https://kafka.apache.org>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Apache Kafka is an open-source distributed event streaming platform sed by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![Click to enlarge](images/Miro-Kafka.jpg){.lightbox}\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kafka Introduction \n",
    "<https://kafka.apache.org/documentation.html#introduction>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is event streaming?\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "> Event streaming is the digital equivalent of the human body's central nervous system. \n",
    "It is the technological foundation for the 'always-on' world where businesses are increasingly software-defined and automated, and where the user of software is more software.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://www.ge.com/news/sites/default/files/Reports/2020-03/nervousgetty_717260284059573363.jpg)\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A digital nervous system\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "> As the boss of Microsoft, the world's most successful software company, I played a large part in the birth of the Information Age. In this book I explain the idea of a digital nervous system—the use of information\n",
    "technology to satisfy people's needs at work and at home...\n",
    "\n",
    "Bill Gates - 1999\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://upload.wikimedia.org/wikipedia/en/3/3d/Business_%40_the_Speed_of_Thought_%28book_cover%29.jpg)\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A more technical definition{background-color=\"white\" background-image=\"https://www.oreilly.com/api/v2/epubs/9781492042563/files/assets/msos_0126.png\" background-size=\"60%\" background-opacity=\"0.3\"}\n",
    "\n",
    "Technically speaking, event streaming is the practice of..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Capture\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "capturing data in real-time from event sources like:\n",
    "\n",
    "- databases\n",
    "- sensors\n",
    "- mobile devices\n",
    "- cloud services\n",
    "- software applications \n",
    "in the form of streams of events; \n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://fieldcode.com/shared/blog/image-thumb__299__area-image-landscape-c8/iot_firehose_inner@2x.jpg)\n",
    "\n",
    "[Reducing costs with improved data capture in the field](https://fieldcode.com/en/resources/blog/reducing-costs-with-improved-data-capture-in-the-field=\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Storage\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "Storing these event streams durably for later retrieval\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://images.unsplash.com/photo-1484662020986-75935d2ebc66?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2070&q=80)\n",
    "[Unsplah](https://unsplash.com/photos/1iVKwElWrPA)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### It's Okay To Store Data In Kafka\n",
    "<https://www.confluent.io/blog/okay-store-data-apache-kafka/>\n",
    "\n",
    "- The question is whether you can treat this log like a file and use it as the source-of-truth store for your data.\n",
    "\n",
    "- Obviously this is possible, if you just set the retention to “forever” or enable log compaction on a topic, then data will be kept for all time. But I think the question people are really asking, is less whether this will work, and more whether it is something that is totally insane to do.\n",
    "\n",
    "- The short answer is that it’s not insane, people do this all the time, and Kafka was actually designed for this type of usage. But first, why might you want to do this? There are actually a number of use cases, here’s a few:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Processing\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "manipulating, processing, and reacting to the event streams in real-time as well as retrospectively\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://i.imgflip.com/7hgo07.jpg)\n",
    "\n",
    "[NicsMeme](https://imgflip.com/i/7hgo07)\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Routing\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "and routing the event streams to different destination technologies as needed. \n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rairMYDqDpTmzytnqpYnQ.jpeg)\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Continuous processing\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Event streaming ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://www.protocol80.com/hs-fs/hub/1547213/file-3362737430-jpg/blog-files/right-people-right-time-1024x382.jpg?width=2048&height=764&name=right-people-right-time-1024x382.jpg)\n",
    "[Source](https://www.protocol80.com/blog/2015/01/16/keywords-for-buyers-journey/)\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kafka as streaming platform has three key capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.\n",
    "\n",
    "\n",
    "![AWS Pub Sub](https://d1.awsstatic.com/product-marketing/Messaging/sns_img_topic.e024462ec88e79ed63d690a2eed6e050e33fb36f.png){.fragment .lightbox}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Store streams of records in a fault-tolerant durable way.\n",
    "<http://tutorials.jenkov.com/data-streaming/index.html>\n",
    "\n",
    "![](http://tutorials.jenkov.com/images/data-streaming/data-streaming-storage-2.png){.fragment .lightbox}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Process streams of records as they occur.\n",
    "<https://towardsdatascience.com/introduction-to-stream-processing-5a6db310f1b4>\n",
    "\n",
    "![](https://miro.medium.com/max/1400/0*Ud9GAwiHAragiaPv){.fragment .lightbox}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kafka is generally used for two broad classes of applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Building real-time streaming data pipelines that reliably get data between systems or applications\n",
    "<https://www.confluent.io/blog/the-future-of-etl-isnt-what-it-used-to-be/>\n",
    "\n",
    "![](https://cdn.confluent.io/wp-content/uploads/streaming_platform_rev-768x343.png){.fragment .lightbox}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Building real-time streaming applications that transform or react to the streams of data\n",
    "\n",
    "<https://www.researchgate.net/publication/333653951_Big_data_stream_analysis_a_systematic_literature_review>\n",
    "\n",
    "![](https://www.researchgate.net/profile/Olawande_Daramola/publication/333653951/figure/fig1/AS:767176877277184@1559920629392/Data-flow-graph-of-a-stream-processor-The-figure-shows-how-applications-made-up-of.png){.fragment .lightbox}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kafka is distribuited \n",
    "Kafka run as a cluster on one or more servers that can span multiple datacenters.\n",
    "\n",
    "<https://eng.uber.com/ureplicator-apache-kafka-replicator/>\n",
    "\n",
    "![](https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2016/08/image00-e1660780487540.png){.fragment .lightbox}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kafka record organization\n",
    "\n",
    "The Kafka cluster stores streams of records in categories called topics.\n",
    "<https://dzone.com/articles/monitoring-kafka-data-pipeline>\n",
    "\n",
    "![](https://dzone.com/storage/temp/7933597-production-1.png){.fragment .lightbox}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kafka Records\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"35%\"}\n",
    "Each record consists of a key, a value, and a timestamp.\n",
    "![](https://miro.medium.com/max/1400/1*4UOYy2WLNt3cQCqMDqCYLA.jpeg){.lightbox}\n",
    "\n",
    "[Source](https://medium.com/swlh/exploit-apache-kafkas-message-format-to-save-storage-and-bandwidth-7e0c533edf26)\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"65%\"}\n",
    "- Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array.\n",
    "- Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses.\n",
    "- Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage.\n",
    "- Messages (aka Records) are always written in batches.\n",
    "- The technical term for a batch of messages is a record batch, and a record batch contains one or more records.\n",
    "- In the degenerate case, we could have a record batch containing a single record. \n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Kafka record header\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "Record batches and records have their own headers.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "```\n",
    "baseOffset: int64\n",
    "batchLength: int32\n",
    "partitionLeaderEpoch: int32\n",
    "magic: int8 (current magic value is 2)\n",
    "crc: int32\n",
    "attributes: int16\n",
    "    bit 0~2:\n",
    "        0: no compression\n",
    "        1: gzip\n",
    "        2: snappy\n",
    "        3: lz4\n",
    "        4: zstd\n",
    "    bit 3: timestampType\n",
    "    bit 4: isTransactional (0 means not transactional)\n",
    "    bit 5: isControlBatch (0 means not a control batch)\n",
    "    bit 6~15: unused\n",
    "lastOffsetDelta: int32\n",
    "firstTimestamp: int64\n",
    "maxTimestamp: int64\n",
    "producerId: int64\n",
    "producerEpoch: int16\n",
    "baseSequence: int32\n",
    "records: [Record]\n",
    "```\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# API\n",
    "\n",
    "![](images/kafka-apis.png){.r-scratch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Producer\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "To publish (write) a stream of events to one or more Kafka topics.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](https://cdn.confluent.io/wp-content/uploads/Screenshot-2017-07-19-19.14.28.png){.lightbox}\n",
    "https://www.confluent.io/blog/apache-kafka-for-service-architectures/\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An example in python: producer\n",
    "\n",
    "<https://towardsdatascience.com/kafka-python-explained-in-10-lines-of-code-800e3e07dad1>\n",
    "\n",
    "```python\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                         value_serializer=lambda x: \n",
    "                         dumps(x).encode('utf-8'))\n",
    "\n",
    "for e in range(1000):\n",
    "    data = {'number' : e}\n",
    "    producer.send('numtest', value=data)\n",
    "    sleep(5)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Consumer\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "To subscribe to (read) one or more topics and to process the stream of events produced to them.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](images/kafka-meme.jpg)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An example in python: consumer\n",
    "\n",
    "<https://towardsdatascience.com/kafka-python-explained-in-10-lines-of-code-800e3e07dad1>\n",
    "\n",
    "```python\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'numtest',\n",
    "     bootstrap_servers=['localhost:9092'],\n",
    "     auto_offset_reset='earliest',\n",
    "     enable_auto_commit=True,\n",
    "     group_id='my-group',\n",
    "     value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
    "\n",
    "for message in consumer:\n",
    "    message = message.value\n",
    "    print('{} read'.format(message))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Streams\n",
    "\n",
    "Allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams\n",
    "\n",
    "(we will see in next lessons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Connector\n",
    "\n",
    "allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.\n",
    "\n",
    "(we will see in next lessons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Communication{background-color=\"black\" background-image=\"images/gimli.gif\" background-size=\"80%\" background-opacity=\"0.5\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How the communication works\n",
    "In Kafka the communication between the clients and the servers is done with a:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**simple**\n",
    "\n",
    "![](images/simple.jfif){.r-scretch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**high-performance**\n",
    "\n",
    "![](images/high-performance.jpg){.r-scretch}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**language agnostic**\n",
    "\n",
    "![](images/language-agnostic.png){.r-scretch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**TCP protocol**\n",
    "\n",
    "![](images/salemove_tcp_rocks.png){.r-scretch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kafka protocol\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "This protocol is versioned and maintains backwards compatibility with older version. \n",
    "\n",
    "We provide a Java client for Kafka, but clients are available in many languages.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](images/kafka-java.jfif){.r-scratch}\n",
    ":::\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "A topic is a category or feed name to which records are published. Topics in Kafka are always multi-subscriber; that is, a topic can have zero, one, or many consumers that subscribe to the data written to it.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](images/log_anatomy.png)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Partition\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "Each partition is an ordered, immutable sequence of records that is continually appended to—a structured commit log. The records in the partitions are each assigned a sequential id number called the offset that uniquely identifies each record within the partition.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "![](images/log_consumer.png)\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Retention\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "The Kafka cluster durably persists all published records—whether or not they have been consumed—using a configurable retention period. For example, if the retention policy is set to two days, then for the two days after a record is published, it is available for consumption, after which it will be discarded to free up space. Kafka's performance is effectively constant with respect to data size so storing data for a long time is not a problem.\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "<https://learn.conduktor.io/kafka/kafka-topic-configuration-log-retention/>\n",
    "![](https://learn.conduktor.io/kafka/_next/image/?url=https%3A%2F%2Fimages.ctfassets.net%2Fo12xgu4mepom%2F0fofNoZVx0UOPLFhWTkvX%2Ff0bf1c4c8c0492a9ae89297c886f03d3%2FAdv_Kafka_Topic_Log_Comp_1.png&w=3840&q=75)\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Producer / Consumer\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "**Producer**\n",
    "\n",
    "Producers publish data to the topics of their choice. The producer is responsible for choosing which record to assign to which partition within the topic. This can be done in a round-robin fashion simply to balance load or it can be done according to some semantic partition function (say based on some key in the record)\n",
    "\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "**Consumer**\n",
    "\n",
    "Consumers label themselves with a consumer group name, and each record published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.\n",
    "\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Same Group or not ?\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "- If all the consumer instances have the same consumer group, then the records will effectively be load balanced over the consumer instances.\n",
    "- If all the consumer instances have different consumer groups, then each record will be broadcast to all the consumer processes.\n",
    "::: \n",
    "\n",
    "::: {.fragment .column width=\"50%\"}\n",
    "\n",
    "**Example**\n",
    "\n",
    "A two server Kafka cluster hosting four partitions (P0-P3) with two consumer groups. Consumer group A has two consumer instances and group B has four.\n",
    "![](images/consumer-groups.png)\n",
    "\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Guarantees{background-color=\"black\" background-image=\"images/garantee.jfif\" background-size=\"70%\" background-opacity=\"0.5\"}\n",
    "\n",
    "- Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a record M1 is sent by the same producer as a record M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.\n",
    "- A consumer instance sees records in the order they are stored in the log.\n",
    "- For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any records committed to the log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Kafka as a Messaging System\n",
    "- Messaging traditionally has two models: queuing and publish-subscribe.\n",
    "- In a queue, a pool of consumers may read from a server and each record goes to one of them; in publish-subscribe the record is broadcast to all consumers\n",
    "- The consumer group concept in Kafka generalizes these two concepts. As with a queue the consumer group allows you to divide up processing over a collection of processes (the members of the consumer group). As with publish-subscribe, Kafka allows you to broadcast messages to multiple consumer groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Kafka does it better\n",
    "\n",
    "- By having a notion of parallelism—the partition—within the topics, **Kafka is able to provide both ordering guarantees and load balancing** over a pool of consumer processes. This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that each partition is consumed by **exactly one** consumer in the group.\n",
    "\n",
    "- By doing this we ensure that the consumer is the **only reader of that partition** and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances.\n",
    "\n",
    "::: {.callout-note}\n",
    "there cannot be more consumer instances in a consumer group than partitions\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Kafka as a Storage System\n",
    "- Any message queue that allows publishing messages decoupled from consuming them is effectively acting as a storage system for the in-flight messages\n",
    "- Data written to Kafka is written to disk and replicated for fault-tolerance. Kafka allows producers to wait on acknowledgement so that a write isn't considered complete until it is fully replicated and guaranteed to persist even if the server written to fails.\n",
    "- As a result of taking storage seriously and allowing the clients to control their read position, you can think of Kafka as a kind of special purpose distributed filesystem dedicated to high-performance, low-latency commit log storage, replication, and propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Kafka as a Stream Processing\n",
    "\n",
    "- It isn't enough to just read, write, and store streams of data, the purpose is to enable real-time processing of streams.\n",
    "- In Kafka a stream processor is anything that takes continual streams of data from input topics, performs some processing on this input, and produces continual streams of data to output topics.\n",
    "- For example, a retail application might take in input streams of sales and shipments, and output a stream of reorders and price adjustments computed off this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## A dedicated API\n",
    "- It is possible to do simple processing directly using the producer and consumer APIs. However for more complex transformations Kafka provides a fully integrated Streams API. This allows building applications that do non-trivial processing that compute aggregations off of streams or join streams together.\n",
    "- This facility helps solve the hard problems this type of application faces: handling out-of-order data, reprocessing input as code changes, performing stateful computations, etc.\n",
    "- The streams API builds on the core primitives Kafka provides: it uses the producer and consumer APIs for input, uses Kafka for stateful storage, and uses the same group mechanism for fault tolerance among the stream processor instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Putting the Pieces Together\n",
    "\n",
    "- This combination of messaging, storage, and stream processing may seem unusual but it is essential to Kafka’s role as a streaming platform.\n",
    "- A distributed file system like HDFS allows storing static files for batch processing. Effectively a system like this allows storing and processing historical data from the past.\n",
    "- A traditional enterprise messaging system allows processing future messages that will arrive after you subscribe. Applications built in this way process future data as it arrives.\n",
    "- Kafka combines both of these capabilities, and the combination is critical both for Kafka usage as a platform for streaming applications as well as for streaming data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Past and future\n",
    "- By combining storage and low-latency subscriptions, streaming applications can treat both **past and future** data the same way.\n",
    "- That is a single application can process historical, stored data but rather than ending when it reaches the last record it can keep processing as future data arrives.\n",
    "- This is a generalized notion of stream processing that subsumes batch processing as well as message-driven applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "## Low latency data pipelines\n",
    "\n",
    "- Likewise for streaming data pipelines the combination of subscription to real-time events make it possible to use Kafka for very low-latency pipelines;\n",
    "- but the ability to store data reliably make it possible to use it for critical data where the delivery of data must be guaranteed or for integration with offline systems that load data only periodically or may go down for extended periods of time for maintenance.\n",
    "- The stream processing facilities make it possible to transform data as it arrives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![](images/demo.){.r-scratch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Quick Start (with Docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Come lo vuoi sto Kafka ?\n",
    "\n",
    "![](https://i.imgflip.com/7gxclt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Start Kafka Server in Docker\n",
    "\n",
    "To start a basic kafka enviroment \n",
    "\n",
    "```bash\n",
    "run --rm -p 9092:9092  --name kafkaServer apache/kafka:3.8.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Create Topic in Docker\n",
    "\n",
    "\n",
    "```bash\n",
    "docker exec --workdir /opt/kafka/bin/ -it kafkaServer sh\n",
    "./kafka-topics.sh --create --topic tap --bootstrap-server localhost:9092\n",
    "./kafka-topics.sh --describe --topic tap --bootstrap-server localhost:9092\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# A console producer/consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Start Producer Console in Docker \n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "docker build ../kafka/ --tag tap:kafka\n",
    "docker run -e KAFKA_ACTION=producer -e KAFKA_TOPIC=tap --network tap  -it tap:kafka\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Start Consumer Console in Docker \n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "docker build ../kafka/ --tag tap:kafka\n",
    "docker run -e KAFKA_ACTION=consumer -e KAFKA_TOPIC=tap --network tap   -it tap:kafka\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Let's start another one\n",
    "What will happen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kafka Real World Example\n",
    "https://www.confluent.io/blog/how-kafka-is-used-by-netflix/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kafka with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DockerFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```DockerFile\n",
    "FROM python\n",
    "ENV PATH /usr/src/app/bin:$PATH\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "COPY requirements.txt ./\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY bin/* ./\n",
    "COPY python-manager.sh /\n",
    "ENTRYPOINT [ \"/python-manager.sh\" ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Requirements\n",
    "List of python packages automatically added to docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "kafka-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## python-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "[[ -z \"${PYTHON_APP}\" ]] && { echo \"PYTHON_APP required\"; exit 1; }\n",
    "PYTHON_DIR=\"/usr/src/app/\"\n",
    "\n",
    "echo \"Running python ${PYTHON_APP} (Python Dir:${PYTHON_DIR})\"\n",
    "cd /usr/src/app/\n",
    "python ${PYTHON_APP}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Build Python Image\n",
    "```bash\n",
    "docker build python --tag tap:python\n",
    "```\n",
    "- Start Zk\n",
    "```bash\n",
    "docker run -e KAFKA_ACTION=start-zk --network tap --ip 10.0.100.22  -p 2181:2181 --name kafkaZK -it tap:kafka\n",
    "```\n",
    "- Start Kafka Server\n",
    "```bash\n",
    "docker run -e KAFKA_ACTION=start-kafka --network tap --ip 10.0.100.23  -p 9092:9092 --name kafkaServer -it tap:kafka\n",
    "```\n",
    "- Start Consumer kafkaPython10linesConsumer\n",
    "```bash\n",
    "docker run --network tap -e PYTHON_APP=tap10linesConsumer.py --rm -it tap:python\n",
    "```\n",
    "\n",
    "- Start Producer kafkaPython10linesProducer\n",
    "```bash\n",
    "docker run --network tap -e PYTHON_APP=tap10linesProducer.py --rm -it tap:python\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's start another Consumer\n",
    "```bash\n",
    "docker run --network tap -e PYTHON_APP=tap10linesConsumer.py --rm -it tap:python\n",
    "```\n",
    "What will happen ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's start nother Consumer with a different group\n",
    "\n",
    "```bash\n",
    "docker run --network tap -e PYTHON_APP=tap10linesConsumer.py -e GROUP_ID=my-group2 --rm -it tap:python\n",
    "```\n",
    "\n",
    "How can I read in parallel ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Increase the number of partitions\n",
    "\n",
    "```bash\n",
    "docker exec -it kafkaServer kafka-topics.sh --alter --bootstrap-server kafkaServer:9092  --topic tap --partitions 2\n",
    "```\n",
    "\n",
    "and run multiple consumer / producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A faster producer\n",
    "```bash\n",
    "docker run --network tap -e PYTHON_APP=tap10linesProducer.py -e pause=0 --rm -it tap:python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### A multithread consumer\n",
    "```bash\n",
    "docker run --network tap -e PYTHON_APP=tapConcurrentRead.py --rm -it tap:python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kafka and Spring Boot\n",
    "https://docs.spring.io/spring-kafka/docs/current/reference/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Kafka as a Destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Docker Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Fluentd\n",
    "![](https://www.fluentd.org/images/recipes/fluentd_docker.png)\n",
    "https://www.fluentd.org/guides/recipes/docker-logging\n",
    "\n",
    "https://sematext.com/guides/docker-logs/\n",
    "\n",
    "https://ridwanfajar.medium.com/send-your-container-logs-to-elk-elasticsearch-logstash-and-kibana-with-gelf-driver-7995714fbbad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "docker build . --tag tap:fluentd\n",
    "docker run --network tap -p 24224:24224 -v $(pwd)/conf:/fluentd/etc tap:fluentd -c /fluentd/etc/in_docker.conf\n",
    "docker run --rm -p 80:80 --network tap --log-driver=fluentd --log-opt tag=\"docker.{{.ID}}\" ubuntu echo 'Hello Fluentd!'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ngnix Logs\n",
    "https://medium.com/interleap/docker-volumes-and-logging-a839011950f7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Create a volume\n",
    "```bash\n",
    "docker volume create ngnix_logs\n",
    "docker volume ls | grep ngnix_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Nginx Run\n",
    "```bash\n",
    "# Need to create a custom image to remove link\n",
    "docker build nginx --tag tap:nginx\n",
    "# to start\n",
    "docker-compose -f ngnix-to-kafka-run.yml up\n",
    "\n",
    "# to stop\n",
    "# CTRL + C and to clean (important for kafka server)\n",
    "docker-compose -f ngnix-to-kafka-run.yml \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- https://www.confluent.io/resources/kafka-the-definitive-guide\n",
    "- https://ordina-jworks.github.io/kafka/2018/10/23/kafka-stream-introduction.html\n",
    "- https://medium.com/better-programming/kafka-docker-run-multiple-kafka-brokers-and-zookeeper-services-in-docker-3ab287056fd5\n",
    "- https://coralogix.com/log-analytics-blog/a-complete-introduction-to-apache-kafka/\n",
    "- https://timber.io/blog/hello-world-in-kafka-using-python/\n",
    "- https://medium.com/@contactsunny/simple-apache-kafka-producer-and-consumer-using-spring-boot-41be672f4e2b\n",
    "- https://towardsdatascience.com/getting-started-with-apache-kafka-in-python-604b3250aa05\n",
    "- https://www.slideshare.net/ConfluentInc/show-me-kafka-tools-that-will-increase-my-productivity-stephane-maarek-datacumulus-kafka-summit-london-2019\n",
    "- https://www.slideshare.net/FlorentRamiere/apache-kafka-patterns-antipatterns\n",
    "- https://medium.com/@stephane.maarek/the-kafka-api-battle-producer-vs-consumer-vs-kafka-connect-vs-kafka-streams-vs-ksql-ef584274c1e\n",
    "- https://medium.com/streamthoughts/understanding-kafka-partition-assignment-strategies-and-how-to-write-your-own-custom-assignor-ebeda1fc06f3"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2024 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
